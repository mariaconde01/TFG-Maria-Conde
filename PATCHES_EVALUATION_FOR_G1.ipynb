{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ir0oFgsO4Yd",
        "outputId": "2e685bc9-4615-4b56-c3c1-c959a14f8e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import binary_fill_holes\n",
        "from skimage.measure import label, regionprops\n",
        "import h5py  # h5py is used for handling HDF5 files\n",
        "import tensorflow as tf  # TensorFlow is a library for machine learning\n",
        "from tensorflow.keras.models import model_from_json  # This imports the function to load Keras models"
      ],
      "metadata": {
        "id": "28ylGn6CO9Zc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Ruta del archivo ZIP en Google Drive\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/DATASET TFG (1).zip'\n",
        "\n",
        "# Directorio donde quieres extraer el contenido\n",
        "extract_dir = '/dataset tfg final'\n",
        "# Crear el directorio si no existe\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Extraer el archivo ZIP\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f'Archivos extraídos en: {extract_dir}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlY8Q_CnO9b5",
        "outputId": "45df00e9-7638-47ac-a2bb-2a7fceac8736"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos extraídos en: /dataset tfg final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Ruta de la carpeta que deseas eliminar\n",
        "folder_path = '/dataset tfg final'\n",
        "\n",
        "# Eliminar la carpeta\n",
        "shutil.rmtree(folder_path)\n",
        "\n",
        "print(f\"Carpeta {folder_path} eliminada con éxito.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT01fnN3Ro4p",
        "outputId": "7d2d911c-9ad9-4b4e-dece-2200ed36e766"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carpeta /dataset tfg final eliminada con éxito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BACKGROUND REMOVAL\n",
        "\n",
        "# Define the input and output folder paths\n",
        "# `input_folder` contains the raw images to be processed.\n",
        "# `output_folder` is where the processed images will be saved.\n",
        "input_folder = '/dataset tfg final'\n",
        "output_folder = '/preprocess final'\n",
        "\n",
        "# Path to store preprocessed images, organized by grade\n",
        "preprocessed_folder = os.path.join(output_folder, 'preprocessed')\n",
        "\n",
        "# Function to determine the grade of an image based on its filename\n",
        "def determine_grade(filename):\n",
        "    \"\"\"\n",
        "    Determines the grade of osteoarthritis based on the filename.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The name of the image file.\n",
        "\n",
        "    Returns:\n",
        "        int: The grade of osteoarthritis (1 for this case), or None if not identifiable.\n",
        "    \"\"\"\n",
        "    if \"Gr1\" in filename:\n",
        "        return 1\n",
        "    else:\n",
        "        return None  # Ignore other grades\n",
        "\n",
        "# Function to process an individual image\n",
        "def process_image(input_path, output_path, filename):\n",
        "    \"\"\"\n",
        "    Processes a single image: removes the background, applies segmentation,\n",
        "    and saves the preprocessed image into a corresponding folder.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The folder path where the image is located.\n",
        "        output_path (str): The folder path where the processed image will be saved.\n",
        "        filename (str): The name of the image file.\n",
        "    \"\"\"\n",
        "    # Full paths for the input and output image\n",
        "    input_file_path = os.path.join(input_path, filename)\n",
        "    output_file_path = os.path.join(output_path, filename)\n",
        "\n",
        "    try:\n",
        "        # Process only specific images (e.g., \"originales\" and \"saf\" in filename)\n",
        "        if \"originales\" in input_path.lower() and (\"saf\" in filename.lower() or \"safo\" in filename.lower()):\n",
        "            # Load the image in color\n",
        "            color_image = cv2.imread(input_file_path)\n",
        "\n",
        "            # Crop the image to a fixed size (e.g., 3072 pixels height)\n",
        "            cropped_image = color_image[:3072, :]\n",
        "\n",
        "            # Convert the image to grayscale\n",
        "            gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Identify black pixels in the grayscale image\n",
        "            black_pixels = (gray_image == 0)\n",
        "\n",
        "            # Create a mask for black pixels\n",
        "            black_pixels_mask = np.zeros_like(gray_image)\n",
        "            black_pixels_mask[black_pixels] = 255\n",
        "\n",
        "            # Dilate the black pixel mask to expand the regions\n",
        "            kernel = np.ones((300, 300), np.uint8)\n",
        "            dilated_mask = cv2.dilate(black_pixels_mask, kernel, iterations=1)\n",
        "\n",
        "            # Identify adjacent white pixels (potential tissue borders)\n",
        "            adjacent_white_pixels = (cv2.dilate(dilated_mask, np.ones((9, 9), np.uint8), iterations=1) - dilated_mask) > 0\n",
        "\n",
        "            # Compute the average intensity of the adjacent white pixels\n",
        "            whitish_tone = np.mean(gray_image[adjacent_white_pixels])\n",
        "\n",
        "            # Handle cases where the computed tone is not finite\n",
        "            if not np.isfinite(whitish_tone):\n",
        "                whitish_tone = 220  # Default value for missing intensity\n",
        "\n",
        "            # Replace the dilated black regions with the whitish tone\n",
        "            gray_image[dilated_mask > 0] = whitish_tone\n",
        "\n",
        "            # Apply Otsu's thresholding to binarize the grayscale image\n",
        "            _, otsu_threshold = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "            # Invert the binary mask to focus on black regions\n",
        "            inverted_otsu_threshold = cv2.bitwise_not(otsu_threshold)\n",
        "\n",
        "            # Fill holes in the binary mask\n",
        "            filled_image = binary_fill_holes(inverted_otsu_threshold).astype(np.uint8) * 255\n",
        "\n",
        "            # Label connected components in the binary mask\n",
        "            labeled_image, num_features = label(filled_image, return_num=True, connectivity=2)\n",
        "\n",
        "            # Get properties of the labeled regions\n",
        "            regions = regionprops(labeled_image)\n",
        "\n",
        "            # Identify the largest connected region by area\n",
        "            if regions:\n",
        "                largest_region = max(regions, key=lambda r: r.area)\n",
        "                largest_region_mask = (labeled_image == largest_region.label).astype(np.uint8) * 255\n",
        "            else:\n",
        "                largest_region_mask = filled_image  # Default to the filled image if no regions are found\n",
        "\n",
        "            # Apply the mask to retain only the largest region in the color image\n",
        "            final_color_image = cv2.bitwise_and(cropped_image, cropped_image, mask=largest_region_mask)\n",
        "\n",
        "            # Determine the grade of the image using the filename\n",
        "            grade = determine_grade(filename)\n",
        "            if grade == 1:  # Only process Grade 1 images\n",
        "                # Create a folder for the specific grade\n",
        "                grade_folder = os.path.join(preprocessed_folder, f'grade{grade}')\n",
        "                os.makedirs(grade_folder, exist_ok=True)\n",
        "\n",
        "                # Save the processed image with a new name\n",
        "                output_filename = f\"processed_{filename}\"\n",
        "                preprocessed_output_path = os.path.join(grade_folder, output_filename)\n",
        "                cv2.imwrite(preprocessed_output_path, final_color_image)  # Save the processed image\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during processing\n",
        "        print(f\"Failed to process the image: {input_file_path}\")\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Remove the output folder if it already exists and create a clean one\n",
        "if os.path.exists(output_folder):\n",
        "    shutil.rmtree(output_folder)  # Delete the folder and its contents\n",
        "os.makedirs(output_folder)\n",
        "os.makedirs(preprocessed_folder)\n",
        "\n",
        "# Walk through the input folder to find all image files\n",
        "for root, folders, files in os.walk(input_folder):\n",
        "    for filename in files:\n",
        "        # Process only specific image formats and Grade 1 images\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff')) and \"Gr1\" in filename:\n",
        "            process_image(root, output_folder, filename)  # Process each valid image\n",
        "\n",
        "print('Process completed.')  # Indicate that the process has finished\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxzQoEtiQP0w",
        "outputId": "e155bca8-9fe1-4b40-e2f4-559bc095e695"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Define the input and output folder paths for image patches\n",
        "preprocessed_folder = '/preprocess final/preprocessed'  # Folder containing all preprocessed images (for all grades)\n",
        "patches_folder = '/content/1emodapatches'  # Main folder for patches\n",
        "patches_output_folder = os.path.join(patches_folder, 'patches')  # Folder where the patches will be saved\n",
        "\n",
        "# Create the output folder for image patches if it doesn't exist\n",
        "os.makedirs(patches_output_folder, exist_ok=True)\n",
        "\n",
        "# Create subfolders for grade1 inside the patches folder (since we're processing only grade1)\n",
        "os.makedirs(os.path.join(patches_output_folder, 'grade1'), exist_ok=True)\n",
        "\n",
        "# Function to create patches from an image\n",
        "def create_patches(image_path, output_folder, patch_size=(3072, 3072), stride=512):\n",
        "    \"\"\"\n",
        "    Splits an image into smaller patches.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        output_folder (str): Folder where the patches will be saved.\n",
        "        patch_size (tuple): The size of each patch (height, width).\n",
        "        stride (int): The step size for sliding the window over the image.\n",
        "\n",
        "    Returns:\n",
        "        None: Patches are saved directly to the output folder.\n",
        "    \"\"\"\n",
        "    # Read the image from the path\n",
        "    image = cv2.imread(image_path)\n",
        "    image_height, image_width, _ = image.shape  # Get image dimensions\n",
        "    patch_count = 0  # Initialize the counter for patches\n",
        "\n",
        "    # Loop through the image to create patches\n",
        "    for y in range(0, image_height, stride):\n",
        "        for x in range(0, image_width, stride):\n",
        "            # Ensure the patch fits within the image boundaries\n",
        "            if y + patch_size[1] <= image_height and x + patch_size[0] <= image_width:\n",
        "                # Extract the patch from the image\n",
        "                patch = image[y:y + patch_size[1], x:x + patch_size[0]]\n",
        "                patch_filename = f'{os.path.splitext(os.path.basename(image_path))[0]}_patch_{patch_count}.png'\n",
        "                patch_output_path = os.path.join(output_folder, patch_filename)\n",
        "                # Save the patch to the output folder\n",
        "                cv2.imwrite(patch_output_path, patch)\n",
        "                patch_count += 1  # Increment the patch counter\n",
        "\n",
        "# Only process images from the \"grade1\" folder\n",
        "grade_folder = os.path.join(preprocessed_folder, 'grade1')  # Only \"grade1\" folder\n",
        "patches_grade_folder = os.path.join(patches_output_folder, 'grade1')  # Corresponding patch folder for grade1\n",
        "\n",
        "# Process images only from the grade1 folder\n",
        "for filename in os.listdir(grade_folder):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff', '.png')):  # Filter image files\n",
        "        img_path = os.path.join(grade_folder, filename)\n",
        "        create_patches(img_path, patches_grade_folder)  # Create patches for the image\n",
        "\n",
        "# Print a confirmation message once all images from grade1 have been patched\n",
        "print('All grade1 images have been patched.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsOZp-l0O9gH",
        "outputId": "ec79c6a5-39cc-4346-fc96-37bc26b0cce6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All grade1 images have been patched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Define the folder containing the image patches\n",
        "patches_folder = '/content/1emodapatches/patches'  # Folder where all patches are stored\n",
        "\n",
        "# Define the minimum tissue percentage required to keep a patch (30% of the patch must contain tissue)\n",
        "min_tissue_percentage = 0.30  # 30% of the patch should contain tissue (non-black pixels)\n",
        "\n",
        "# Function to calculate the percentage of tissue in a patch\n",
        "def calculate_tissue_percentage(patch):\n",
        "    \"\"\"\n",
        "    This function calculates the percentage of tissue (non-black pixels) in an image patch.\n",
        "\n",
        "    Args:\n",
        "        patch (numpy array): The input image patch to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        float: The percentage of tissue (non-black pixels) in the patch.\n",
        "    \"\"\"\n",
        "    # Convert the patch to grayscale for easier processing\n",
        "    gray_patch = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply a binary threshold to detect tissue (non-black pixels)\n",
        "    _, thresholded_patch = cv2.threshold(gray_patch, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Count the number of tissue (non-black) pixels\n",
        "    tissue_pixels = np.sum(thresholded_patch == 255)\n",
        "\n",
        "    # Calculate the total number of pixels in the patch\n",
        "    total_pixels = thresholded_patch.size\n",
        "\n",
        "    # Compute the tissue percentage\n",
        "    tissue_percentage = tissue_pixels / total_pixels\n",
        "\n",
        "    return tissue_percentage\n",
        "\n",
        "# Function to remove patches with insufficient tissue (less than the specified threshold)\n",
        "def remove_patches_with_little_tissue(patch_folder):\n",
        "    \"\"\"\n",
        "    This function removes patches from a folder if their tissue percentage is below the defined threshold.\n",
        "\n",
        "    Args:\n",
        "        patch_folder (str): The folder containing the patches (organized by grades).\n",
        "    \"\"\"\n",
        "    # Only process images in the \"grade1\" folder\n",
        "    grade_folder = os.path.join(patch_folder, 'grade1')  # Only process grade1\n",
        "\n",
        "    # Iterate over all files in the grade1 folder\n",
        "    for filename in os.listdir(grade_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff', '.png')):  # Process image files only\n",
        "            patch_path = os.path.join(grade_folder, filename)  # Full path to the patch\n",
        "\n",
        "            # Read the patch image\n",
        "            patch = cv2.imread(patch_path)\n",
        "\n",
        "            # Calculate the tissue percentage in the patch\n",
        "            tissue_percentage = calculate_tissue_percentage(patch)\n",
        "\n",
        "            # If the tissue percentage is below the defined threshold, remove the patch\n",
        "            if tissue_percentage < min_tissue_percentage:\n",
        "                os.remove(patch_path)  # Delete the patch\n",
        "                print(f'Removed patch: {patch_path} (tissue percentage: {tissue_percentage:.2f})')\n",
        "\n",
        "# Call the function to remove patches with insufficient tissue from the grade1 folder only\n",
        "remove_patches_with_little_tissue(patches_folder)\n",
        "\n",
        "# Print a confirmation message once the process is complete\n",
        "print('Patches with insufficient tissue have been removed from grade1.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1fBVSdFO9kF",
        "outputId": "9a3c6f72-9662-4511-e026-67e5a6a3f46e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed patch: /content/1emodapatches/patches/grade1/processed_Gr1_Saf_053_patch_0.png (tissue percentage: 0.20)\n",
            "Removed patch: /content/1emodapatches/patches/grade1/processed_Gr1_Saf_074_patch_13.png (tissue percentage: 0.27)\n",
            "Patches with insufficient tissue have been removed from grade1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Define the folder where all previously generated image patches are located\n",
        "patches_folder = '/content/1emodapatches/patches'  # Folder containing the patches\n",
        "\n",
        "# Define the output folder where the resized patches will be saved\n",
        "resized_folder = '/content/1resizedmoda'  # Output folder for resized patches\n",
        "\n",
        "# Define the target output size (512x512 pixels)\n",
        "output_size = (512, 512)\n",
        "\n",
        "# Create the output folder if it doesn't already exist\n",
        "os.makedirs(resized_folder, exist_ok=True)\n",
        "\n",
        "# Function to resize an image\n",
        "def resize_image(image, size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Resizes the input image to the specified size using INTER_AREA resampling method.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array): The image to be resized.\n",
        "        size (tuple): The target size to resize the image to.\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The resized image.\n",
        "    \"\"\"\n",
        "    # Using INTER_AREA interpolation for resampling (better for downsampling)\n",
        "    resized_image = cv2.resize(image, size, interpolation=cv2.INTER_AREA)\n",
        "    return resized_image\n",
        "\n",
        "# Function to resize all patches in a given folder\n",
        "def resize_patches_in_folder(patch_folder, resized_folder):\n",
        "    \"\"\"\n",
        "    Resizes all image patches in the specified folder and saves them to the resized folder.\n",
        "    The patches are resized to the target size (512x512 pixels).\n",
        "\n",
        "    Args:\n",
        "        patch_folder (str): The folder containing the original image patches.\n",
        "        resized_folder (str): The folder where the resized patches will be stored.\n",
        "    \"\"\"\n",
        "    # Only process the \"grade1\" folder\n",
        "    grade = 1  # Only process grade1\n",
        "    grade_folder = os.path.join(patch_folder, f'grade{grade}')  # Path to the \"grade1\" folder\n",
        "    resized_grade_folder = os.path.join(resized_folder, f'grade{grade}')  # Path to the resized \"grade1\" folder\n",
        "\n",
        "    # Create the resized \"grade1\" folder if it doesn't exist\n",
        "    os.makedirs(resized_grade_folder, exist_ok=True)\n",
        "\n",
        "    # Loop through all image patches in the \"grade1\" folder\n",
        "    for filename in os.listdir(grade_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff', '.png')):  # Process only image files\n",
        "            patch_path = os.path.join(grade_folder, filename)  # Full path to the current patch\n",
        "\n",
        "            # Read the patch image\n",
        "            patch = cv2.imread(patch_path)\n",
        "\n",
        "            if patch is not None:\n",
        "                # Resize the image (resample it to the target size)\n",
        "                resized_patch = resize_image(patch, output_size)\n",
        "\n",
        "                # Save the resized image in the resized folder\n",
        "                resized_patch_path = os.path.join(resized_grade_folder, f'resized_{filename}')\n",
        "                cv2.imwrite(resized_patch_path, resized_patch)  # Save the resized image\n",
        "                print(f'Resized and saved: {resized_patch_path}')  # Print the path of the resized image\n",
        "            else:\n",
        "                print(f'Error reading image: {patch_path}')  # Handle error if the patch cannot be read\n",
        "\n",
        "# Resize all patches in the \"grade1\" folder and save to the resized folder\n",
        "resize_patches_in_folder(patches_folder, resized_folder)\n",
        "\n",
        "# Print a confirmation message once the process is complete\n",
        "print('All grade1 images have been resized to 512x512 and saved to the resized folders.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oMZkOSmO9oz",
        "outputId": "384d0594-cebc-40e9-afbd-830e2eb03483"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_057_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_120_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_8.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_7.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_082_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_058_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_085_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_085_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_8.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_127_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_10.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_082_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_058_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_085_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_082_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_058_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_057_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_058_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_127_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_111_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_111_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_123_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_123_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_120_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_099_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_099_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_7.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_111_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_085_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_099_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_107_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_7.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_107_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_7.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_085_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_103_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_127_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_7.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_11.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_111_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_120_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_099_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_123_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_7.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_067_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_120_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_8.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_082_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_107_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_107_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_082_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_099_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_116_patch_7.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_3.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_123_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_12.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_053_patch_9.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_120_patch_4.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_099_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_099_patch_6.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_8.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_111_patch_2.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_1.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_062_patch_5.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_093_patch_0.png\n",
            "Resized and saved: /content/1resizedmoda/grade1/resized_processed_Gr1_Saf_074_patch_9.png\n",
            "All grade1 images have been resized to 512x512 and saved to the resized folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Keras model from an H5 file\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Ph5 y json/E15model_sin_finetunningdensenet121.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blcswy5yVGie",
        "outputId": "ce5e415b-3627-420d-e2dc-a47ff4c4ba18"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the ImageDataGenerator class from Keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Importing tools from scikit-learn for evaluating the model\n",
        "from sklearn.metrics import classification_report, confusion_matrix  # Used for generating performance metrics (report and confusion matrix)\n",
        "\n",
        "# Create an instance of ImageDataGenerator for image rescaling (normalization)\n",
        "datagen = ImageDataGenerator(rescale=1./255)  # Rescales image pixel values to the range [0, 1]\n",
        "\n",
        "# Create a generator that will load images from the specified directory\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    '/content/1resizedmoda',  # Path to the test dataset\n",
        "    target_size=(512, 512),  # Resizes all images to the target size (512x512 pixels)\n",
        "    batch_size=32,  # Number of images to process in each batch\n",
        "    class_mode='categorical',  # Specifies the format of the labels (for multi-class classification)\n",
        "    shuffle=False  # Don't shuffle the data, as this is for evaluation on the test set\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6MBB9WJVGlF",
        "outputId": "eecbe2fd-591d-4fb0-dbfc-65dbfc696bb7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 108 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_names = []\n",
        "# Obtain predictions in small batches to avoid memory issues\n",
        "y_pred = []  # List to store predicted labels\n",
        "y_true = []  # List to store true labels\n",
        "\n",
        "# Loop through the test set and make predictions in batches\n",
        "for i in range(len(test_generator)):\n",
        "    X, y = test_generator[i]  # Get a batch of data (images and labels)\n",
        "    y_pred.extend(np.argmax(model.predict(X), axis=1))  # Predict the class with the highest probability\n",
        "    y_true.extend(np.argmax(y, axis=1))  # Get the true labels (index of the highest value)\n",
        "    batch_image_names = test_generator.filenames[i * test_generator.batch_size : (i + 1) * test_generator.batch_size]\n",
        "    image_names.extend(batch_image_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywpX1B-nVGnp",
        "outputId": "0ea52af8-bd85-48c6-fe1c-5787cf418873"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 40s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 35s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 32s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict  # Import defaultdict to keep track of counts and lists for each image\n",
        "\n",
        "# `image_patch_count` will store the number of patches for each image\n",
        "# `image_predictions` will store the predictions for each image, indexed by the image name\n",
        "image_patch_count = defaultdict(int)  # Initialize the dictionary to count patches per image (default value is 0)\n",
        "image_predictions = defaultdict(list)  # Initialize the dictionary to store predictions per image (default value is an empty list)\n",
        "\n",
        "# Initialize an index variable to iterate through the patch names and corresponding predictions\n",
        "index = 0\n",
        "\n",
        "# Loop over each patch name in `image_names`\n",
        "for patch_name in image_names:\n",
        "    # Extract the image name by removing the last part (patch number) from the patch name\n",
        "    # Assumes the patch name format is \"imageName_patchNumber\", so we split by \"_\" and join everything except the last part\n",
        "    image_name = \"_\".join(patch_name.split(\"_\")[:-1])\n",
        "\n",
        "    # Increment the patch count for the current image\n",
        "    image_patch_count[image_name] += 1\n",
        "\n",
        "    # Append the prediction for the current patch to the list of predictions for this image\n",
        "    image_predictions[image_name].append(y_pred[index])\n",
        "\n",
        "    # Move to the next prediction (increment the index)\n",
        "    index += 1\n"
      ],
      "metadata": {
        "id": "bVLCj-0FWTPR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_name, preds in image_predictions.items():\n",
        "    print(f'Image: {image_name}')\n",
        "    print(f'Predictions: {preds}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpCqP0z6WTRv",
        "outputId": "8889ce82-59e5-4621-8627-4a1d789dcf7e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: grade1/resized_processed_Gr1_Saf_053_patch\n",
            "Predictions: [0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
            "Image: grade1/resized_processed_Gr1_Saf_057_patch\n",
            "Predictions: [2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_058_patch\n",
            "Predictions: [2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_062_patch\n",
            "Predictions: [0, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_067_patch\n",
            "Predictions: [2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_074_patch\n",
            "Predictions: [2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_082_patch\n",
            "Predictions: [2, 1, 1, 1, 1]\n",
            "Image: grade1/resized_processed_Gr1_Saf_085_patch\n",
            "Predictions: [1, 1, 1, 1, 1]\n",
            "Image: grade1/resized_processed_Gr1_Saf_093_patch\n",
            "Predictions: [2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_099_patch\n",
            "Predictions: [2, 2, 2, 2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_103_patch\n",
            "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_107_patch\n",
            "Predictions: [2, 2, 2, 2]\n",
            "Image: grade1/resized_processed_Gr1_Saf_111_patch\n",
            "Predictions: [1, 1, 1, 1, 1]\n",
            "Image: grade1/resized_processed_Gr1_Saf_116_patch\n",
            "Predictions: [1, 1, 1, 1, 1, 1, 0, 0]\n",
            "Image: grade1/resized_processed_Gr1_Saf_120_patch\n",
            "Predictions: [2, 2, 1, 1, 1]\n",
            "Image: grade1/resized_processed_Gr1_Saf_123_patch\n",
            "Predictions: [0, 1, 1, 1]\n",
            "Image: grade1/resized_processed_Gr1_Saf_127_patch\n",
            "Predictions: [1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVHdroGTWTUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkaNqMbGWTWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2cDFEzmuWTZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4GqghuyBVGqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}