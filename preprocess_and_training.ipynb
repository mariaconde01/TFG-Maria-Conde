{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tykn4nqPWB8O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import binary_fill_holes\n",
        "from skimage.measure import label, regionprops\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import json\n",
        "import random\n",
        "import h5py  # h5py is used for handling HDF5 files\n",
        "from tensorflow.keras.models import model_from_json  # This imports the function to load Keras models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BACKGROUND REMOVAL"
      ],
      "metadata": {
        "id": "qznL2HPxZhLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BACKGROUND REMOVAL\n",
        "\n",
        "# Define the input and output folder paths\n",
        "# `input_folder` contains the raw images to be processed.\n",
        "# `output_folder` is where the processed images will be saved.\n",
        "input_folder = '/dataset tfg final'\n",
        "output_folder = '/preprocess final'\n",
        "\n",
        "# Path to store preprocessed images, organized by grade\n",
        "preprocessed_folder = os.path.join(output_folder, 'preprocessed')\n",
        "\n",
        "# Function to determine the grade of an image based on its filename\n",
        "def determine_grade(filename):\n",
        "    \"\"\"\n",
        "    Determines the grade of osteoarthritis based on the filename.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The name of the image file.\n",
        "\n",
        "    Returns:\n",
        "        int: The grade of osteoarthritis (0, 1, 2, or 3), or None if not identifiable.\n",
        "    \"\"\"\n",
        "    if \"Gr0\" in filename:\n",
        "        return 0\n",
        "    elif \"Gr1\" in filename:\n",
        "        return 1\n",
        "    elif \"Gr2\" in filename:\n",
        "        return 2\n",
        "    elif \"Gr3\" in filename:\n",
        "        return 3\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to process an individual image\n",
        "def process_image(input_path, output_path, filename):\n",
        "    \"\"\"\n",
        "    Processes a single image: removes the background, applies segmentation,\n",
        "    and saves the preprocessed image into a corresponding folder.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): The folder path where the image is located.\n",
        "        output_path (str): The folder path where the processed image will be saved.\n",
        "        filename (str): The name of the image file.\n",
        "    \"\"\"\n",
        "    # Full paths for the input and output image\n",
        "    input_file_path = os.path.join(input_path, filename)\n",
        "    output_file_path = os.path.join(output_path, filename)\n",
        "\n",
        "    try:\n",
        "        # Process only specific images (e.g., \"originales\" and \"saf\" in filename)\n",
        "        if \"originales\" in input_path.lower() and (\"saf\" in filename.lower() or \"safo\" in filename.lower()):\n",
        "            # Load the image in color\n",
        "            color_image = cv2.imread(input_file_path)\n",
        "\n",
        "            # Crop the image to a fixed size (e.g., 3072 pixels height)\n",
        "            cropped_image = color_image[:3072, :]\n",
        "\n",
        "            # Convert the image to grayscale\n",
        "            gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Identify black pixels in the grayscale image\n",
        "            black_pixels = (gray_image == 0)\n",
        "\n",
        "            # Create a mask for black pixels\n",
        "            black_pixels_mask = np.zeros_like(gray_image)\n",
        "            black_pixels_mask[black_pixels] = 255\n",
        "\n",
        "            # Dilate the black pixel mask to expand the regions\n",
        "            kernel = np.ones((300, 300), np.uint8)\n",
        "            dilated_mask = cv2.dilate(black_pixels_mask, kernel, iterations=1)\n",
        "\n",
        "            # Identify adjacent white pixels (potential tissue borders)\n",
        "            adjacent_white_pixels = (cv2.dilate(dilated_mask, np.ones((9, 9), np.uint8), iterations=1) - dilated_mask) > 0\n",
        "\n",
        "            # Compute the average intensity of the adjacent white pixels\n",
        "            whitish_tone = np.mean(gray_image[adjacent_white_pixels])\n",
        "\n",
        "            # Handle cases where the computed tone is not finite\n",
        "            if not np.isfinite(whitish_tone):\n",
        "                whitish_tone = 220  # Default value for missing intensity\n",
        "\n",
        "            # Replace the dilated black regions with the whitish tone\n",
        "            gray_image[dilated_mask > 0] = whitish_tone\n",
        "\n",
        "            # Apply Otsu's thresholding to binarize the grayscale image\n",
        "            _, otsu_threshold = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "            # Invert the binary mask to focus on black regions\n",
        "            inverted_otsu_threshold = cv2.bitwise_not(otsu_threshold)\n",
        "\n",
        "            # Fill holes in the binary mask\n",
        "            filled_image = binary_fill_holes(inverted_otsu_threshold).astype(np.uint8) * 255\n",
        "\n",
        "            # Label connected components in the binary mask\n",
        "            labeled_image, num_features = label(filled_image, return_num=True, connectivity=2)\n",
        "\n",
        "            # Get properties of the labeled regions\n",
        "            regions = regionprops(labeled_image)\n",
        "\n",
        "            # Identify the largest connected region by area\n",
        "            if regions:\n",
        "                largest_region = max(regions, key=lambda r: r.area)\n",
        "                largest_region_mask = (labeled_image == largest_region.label).astype(np.uint8) * 255\n",
        "            else:\n",
        "                largest_region_mask = filled_image  # Default to the filled image if no regions are found\n",
        "\n",
        "            # Apply the mask to retain only the largest region in the color image\n",
        "            final_color_image = cv2.bitwise_and(cropped_image, cropped_image, mask=largest_region_mask)\n",
        "\n",
        "            # Visualize intermediate steps (optional, for debugging or demonstration)\n",
        "            plt.figure(figsize=(18, 6))\n",
        "            plt.subplot(1, 4, 1)\n",
        "            plt.imshow(cv2.cvtColor(otsu_threshold, cv2.COLOR_BGR2RGB))\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Otsu Threshold\")\n",
        "\n",
        "            plt.subplot(1, 4, 2)\n",
        "            plt.imshow(filled_image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Filled Holes\")\n",
        "\n",
        "            plt.subplot(1, 4, 3)\n",
        "            plt.imshow(largest_region_mask, cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Largest Region Mask\")\n",
        "\n",
        "            plt.subplot(1, 4, 4)\n",
        "            plt.imshow(cv2.cvtColor(final_color_image, cv2.COLOR_BGR2RGB))\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Final Color Image\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()  # Display the processed images\n",
        "\n",
        "            # Determine the grade of the image using the filename\n",
        "            grade = determine_grade(filename)\n",
        "            if grade is not None:\n",
        "                # Create a folder for the specific grade\n",
        "                grade_folder = os.path.join(preprocessed_folder, f'grade{grade}')\n",
        "                os.makedirs(grade_folder, exist_ok=True)\n",
        "\n",
        "                # Save the processed image with a new name\n",
        "                output_filename = f\"processed_{filename}\"\n",
        "                preprocessed_output_path = os.path.join(grade_folder, output_filename)\n",
        "                cv2.imwrite(preprocessed_output_path, final_color_image)  # Save the processed image\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during processing\n",
        "        print(f\"Failed to process the image: {input_file_path}\")\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Remove the output folder if it already exists and create a clean one\n",
        "if os.path.exists(output_folder):\n",
        "    shutil.rmtree(output_folder)  # Delete the folder and its contents\n",
        "os.makedirs(output_folder)\n",
        "os.makedirs(preprocessed_folder)\n",
        "\n",
        "# Walk through the input folder to find all image files\n",
        "for root, folders, files in os.walk(input_folder):\n",
        "    for filename in files:\n",
        "        # Process only specific image formats\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "            process_image(root, output_folder, filename)  # Process each valid image\n",
        "\n",
        "print('Process completed.')  # Indicate that the process has finished\n",
        "\n"
      ],
      "metadata": {
        "id": "eh2j0D4ncMd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BALANCE DATASET AND TRAIN/TEST SEPARATION"
      ],
      "metadata": {
        "id": "JnZxwZUWZ-Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BALANCE DATASET AND TRAIN/TEST SEPARATION\n",
        "\n",
        "# Define the input and output folder paths\n",
        "# `input_folder` contains preprocessed images.\n",
        "# `output_folder` is where the balanced train and test sets will be stored.\n",
        "input_folder = '/preprocess final'\n",
        "output_folder = '/safo balanced final'\n",
        "\n",
        "# Define subfolders for train and test splits\n",
        "train_folder = os.path.join(output_folder, 'train')\n",
        "test_folder = os.path.join(output_folder, 'test')\n",
        "\n",
        "# Function to determine the grade of an image based on its filename\n",
        "def determine_grade(filename):\n",
        "    \"\"\"\n",
        "    Determines the grade of osteoarthritis based on the filename.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Name of the image file.\n",
        "\n",
        "    Returns:\n",
        "        int: The grade of osteoarthritis (0, 1, 2, or 3), or None if not identifiable.\n",
        "    \"\"\"\n",
        "    if \"Gr0\" in filename:\n",
        "        return 0\n",
        "    elif \"Gr1\" in filename:\n",
        "        return 1\n",
        "    elif \"Gr2\" in filename:\n",
        "        return 2\n",
        "    elif \"Gr3\" in filename:\n",
        "        return 3\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Create the output folders for train and test sets\n",
        "# Ensures that the required folders exist for each grade in train and test splits.\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "# Create subfolders for each grade (grade0, grade1, grade2, grade3)\n",
        "for grade in range(4):\n",
        "    os.makedirs(os.path.join(train_folder, f'grade{grade}'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_folder, f'grade{grade}'), exist_ok=True)\n",
        "\n",
        "# Traverse all images in the input folder\n",
        "# Collect all image file paths from the input folder.\n",
        "image_files = []\n",
        "for root, folders, files in os.walk(input_folder):\n",
        "    for filename in files:\n",
        "        # Only include specific image file formats\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "            image_files.append(os.path.join(root, filename))\n",
        "\n",
        "# Dictionary to store images grouped by grade\n",
        "# Keys are grades (0, 1, 2, 3), values are lists of image paths.\n",
        "images_by_grade = {0: [], 1: [], 2: [], 3: []}\n",
        "\n",
        "# Categorize images into grades based on their filenames\n",
        "for image_path in image_files:\n",
        "    filename = os.path.basename(image_path)  # Extract just the filename from the full path\n",
        "    grade = determine_grade(filename)  # Determine the grade using the filename\n",
        "    if grade is not None:\n",
        "        images_by_grade[grade].append(image_path)  # Add the image to the corresponding grade list\n",
        "\n",
        "# Balance and split the dataset into train and test sets for each grade\n",
        "for grade, images in images_by_grade.items():\n",
        "    random.shuffle(images)  # Shuffle the images randomly to ensure a balanced split\n",
        "\n",
        "    # Define train and test splits for each grade based on specific rules\n",
        "    if grade == 0:  # Grade 0: 5 images for train, 1 for test\n",
        "        train_images = images[:5]\n",
        "        test_images = images[5:6]\n",
        "    elif grade == 1:  # Grade 1: 7 images for train, rest for test\n",
        "        train_images = images[:7]\n",
        "        test_images = images[7:]\n",
        "    elif grade == 2:  # Grade 2: 7 images for train, rest for test\n",
        "        train_images = images[:7]\n",
        "        test_images = images[7:]\n",
        "    elif grade == 3:  # Grade 3: 4 images for train, 1 for test\n",
        "        train_images = images[:4]\n",
        "        test_images = images[4:5]\n",
        "\n",
        "    # Copy the images to their respective train and test folders\n",
        "    for img_path in train_images:\n",
        "        shutil.copy(img_path, os.path.join(train_folder, f'grade{grade}'))  # Copy to train folder\n",
        "    for img_path in test_images:\n",
        "        shutil.copy(img_path, os.path.join(test_folder, f'grade{grade}'))  # Copy to test folder\n",
        "\n",
        "# Print a confirmation message once the process is complete\n",
        "print('Dataset balanced and split into train and test sets.')\n",
        "\n"
      ],
      "metadata": {
        "id": "a9PPsuT1WCxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PATCHES"
      ],
      "metadata": {
        "id": "8a9BDOkUZpn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGE PATCH CREATION\n",
        "\n",
        "# Define the input and output folder paths for image patches\n",
        "# `patches_train_folder` is the folder where training image patches will be saved.\n",
        "# `patches_test_folder` is the folder where testing image patches will be saved.\n",
        "patches_train_folder = '/safo balanced final/trainpatches'\n",
        "patches_test_folder = '/safo balanced final/testpatches'\n",
        "\n",
        "# Create the output folders for storing patches\n",
        "# Ensures that the required folders exist for patches for both training and testing sets.\n",
        "os.makedirs(patches_train_folder, exist_ok=True)\n",
        "os.makedirs(patches_test_folder, exist_ok=True)\n",
        "\n",
        "# Create subfolders for each grade (grade0, grade1, grade2, grade3)\n",
        "for grade in range(4):\n",
        "    os.makedirs(os.path.join(patches_train_folder, f'grade{grade}'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(patches_test_folder, f'grade{grade}'), exist_ok=True)\n",
        "\n",
        "# Function to create patches from an image\n",
        "def create_patches(image_path, output_folder, patch_size=(3072, 3072), stride=512):\n",
        "    \"\"\"\n",
        "    Splits an image into smaller patches and saves them to the specified folder.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        output_folder (str): The folder where the patches will be saved.\n",
        "        patch_size (tuple): The size of each patch (height, width).\n",
        "        stride (int): The step size for moving the patch window.\n",
        "    \"\"\"\n",
        "    # Read the input image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Get the dimensions of the image\n",
        "    image_height, image_width, _ = image.shape\n",
        "    patch_count = 0  # Counter for naming the patches\n",
        "\n",
        "    # Iterate over the image using the defined stride\n",
        "    for y in range(0, image_height, stride):\n",
        "        for x in range(0, image_width, stride):\n",
        "            # Ensure the patch stays within the image boundaries\n",
        "            if y + patch_size[1] <= image_height and x + patch_size[0] <= image_width:\n",
        "                # Extract the patch\n",
        "                patch = image[y:y + patch_size[1], x:x + patch_size[0]]\n",
        "\n",
        "                # Define the filename for the patch\n",
        "                patch_filename = f'{os.path.splitext(os.path.basename(image_path))[0]}_patch_{patch_count}.png'\n",
        "\n",
        "                # Save the patch to the output folder\n",
        "                patch_output_path = os.path.join(output_folder, patch_filename)\n",
        "                cv2.imwrite(patch_output_path, patch)\n",
        "\n",
        "                # Increment the patch counter\n",
        "                patch_count += 1\n",
        "\n",
        "# Create patches for training images\n",
        "# Iterates through each grade folder in the training set and creates patches for each image.\n",
        "for grade in range(4):\n",
        "    train_grade_folder = os.path.join(train_folder, f'grade{grade}')\n",
        "    patches_train_grade_folder = os.path.join(patches_train_folder, f'grade{grade}')\n",
        "\n",
        "    # Process each image in the current grade folder\n",
        "    for filename in os.listdir(train_grade_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff', '.png')):  # Filter supported image formats\n",
        "            img_path = os.path.join(train_grade_folder, filename)\n",
        "            create_patches(img_path, patches_train_grade_folder)  # Create patches for the image\n",
        "\n",
        "# Create patches for testing images\n",
        "# Similar to the training set, but operates on the test set.\n",
        "for grade in range(4):\n",
        "    test_grade_folder = os.path.join(test_folder, f'grade{grade}')\n",
        "    patches_test_grade_folder = os.path.join(patches_test_folder, f'grade{grade}')\n",
        "\n",
        "    # Process each image in the current grade folder\n",
        "    for filename in os.listdir(test_grade_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff', '.png')):  # Filter supported image formats\n",
        "            img_path = os.path.join(test_grade_folder, filename)\n",
        "            create_patches(img_path, patches_test_grade_folder)  # Create patches for the image\n",
        "\n",
        "# Print a confirmation message\n",
        "print('All images have been patched.')\n"
      ],
      "metadata": {
        "id": "CNJWXJFpWCzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVE PATCHES WITH LESS THAN 30% OF TISSUE\n"
      ],
      "metadata": {
        "id": "VOLsSBswgaPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE PATCHES WITH LESS THAN 30% TISSUE\n",
        "\n",
        "# Define the input folder paths for training and testing image patches\n",
        "# `patches_train_folder` contains the patches from the training set.\n",
        "# `patches_test_folder` contains the patches from the testing set.\n",
        "patches_train_folder = '/safo balanced final/trainpatches'\n",
        "patches_test_folder = '/safo balanced final/testpatches'\n",
        "\n",
        "# Define the minimum percentage of tissue (non-black pixels) required to keep a patch\n",
        "min_tissue_percentage = 0.30  # 30% of the patch must contain tissue\n",
        "\n",
        "# Function to calculate the percentage of tissue in a patch\n",
        "def calculate_tissue_percentage(patch):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of tissue (non-black pixels) in an image patch.\n",
        "\n",
        "    Args:\n",
        "        patch (numpy array): The input image patch.\n",
        "\n",
        "    Returns:\n",
        "        float: The percentage of the patch containing tissue.\n",
        "    \"\"\"\n",
        "    # Convert the patch to grayscale for easier processing\n",
        "    gray_patch = cv2.cvtColor(patch, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply a binary threshold to identify non-black pixels (tissue)\n",
        "    _, thresholded_patch = cv2.threshold(gray_patch, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Count the number of tissue pixels (non-black)\n",
        "    tissue_pixels = np.sum(thresholded_patch == 255)\n",
        "\n",
        "    # Calculate the total number of pixels in the patch\n",
        "    total_pixels = thresholded_patch.size\n",
        "\n",
        "    # Compute the tissue percentage\n",
        "    tissue_percentage = tissue_pixels / total_pixels\n",
        "\n",
        "    return tissue_percentage\n",
        "\n",
        "# Function to remove patches with insufficient tissue\n",
        "def remove_patches_with_little_tissue(patch_folder):\n",
        "    \"\"\"\n",
        "    Removes patches from a folder if their tissue percentage is below the defined threshold.\n",
        "\n",
        "    Args:\n",
        "        patch_folder (str): The folder containing patches organized by grades.\n",
        "    \"\"\"\n",
        "    for grade in range(4):  # Iterate through each grade folder (grade0, grade1, grade2, grade3)\n",
        "        grade_folder = os.path.join(patch_folder, f'grade{grade}')\n",
        "\n",
        "        for filename in os.listdir(grade_folder):\n",
        "            if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff', '.png')):  # Filter image files\n",
        "                patch_path = os.path.join(grade_folder, filename)\n",
        "\n",
        "                # Read the patch image\n",
        "                patch = cv2.imread(patch_path)\n",
        "\n",
        "                # Calculate the percentage of tissue in the patch\n",
        "                tissue_percentage = calculate_tissue_percentage(patch)\n",
        "\n",
        "                # Remove the patch if the tissue percentage is below the threshold\n",
        "                if tissue_percentage < min_tissue_percentage:\n",
        "                    os.remove(patch_path)  # Delete the patch\n",
        "                    print(f'Removed patch: {patch_path} (tissue percentage: {tissue_percentage:.2f})')\n",
        "\n",
        "# Remove patches with insufficient tissue from both training and testing sets\n",
        "remove_patches_with_little_tissue(patches_train_folder)\n",
        "remove_patches_with_little_tissue(patches_test_folder)\n",
        "\n",
        "# Print a confirmation message once the process is complete\n",
        "print('Patches with insufficient tissue have been removed.')\n"
      ],
      "metadata": {
        "id": "7142PVuNWC1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PATCHES BALANCE"
      ],
      "metadata": {
        "id": "lxmYJDR3hjz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for the training and testing patches\n",
        "# `train_dir` contains the patches for training.\n",
        "# `test_dir` contains the patches for testing.\n",
        "train_dir = '/safo balanced final/trainpatches'\n",
        "test_dir = '/safo balanced final/testpatches'\n",
        "\n",
        "# Desired percentages for each grade\n",
        "# These percentages define how much of each grade's images will be moved from training to testing.\n",
        "percentages = {\n",
        "    'grade0': 0.20,  # Move 20% of Grade 0 patches to the test set\n",
        "    'grade1': 0.30,  # Move 30% of Grade 1 patches to the test set\n",
        "    'grade2': 0.30,  # Move 30% of Grade 2 patches to the test set\n",
        "    'grade3': 0.20   # Move 20% of Grade 3 patches to the test set\n",
        "}\n",
        "\n",
        "# Function to move images from one folder to another\n",
        "def move_images(src_folder, dest_folder, percentage):\n",
        "    \"\"\"\n",
        "    Moves a percentage of images from the source folder to the destination folder.\n",
        "\n",
        "    Args:\n",
        "        src_folder (str): Path to the source folder containing the images.\n",
        "        dest_folder (str): Path to the destination folder where images will be moved.\n",
        "        percentage (float): Percentage of images to move (value between 0 and 1).\n",
        "    \"\"\"\n",
        "    # Get the list of image filenames in the source folder\n",
        "    images = os.listdir(src_folder)\n",
        "\n",
        "    # Calculate the number of images to move based on the percentage\n",
        "    num_images_to_move = int(len(images) * percentage)\n",
        "\n",
        "    # Randomly select the images to move\n",
        "    images_to_move = random.sample(images, num_images_to_move)\n",
        "\n",
        "    # Move the selected images to the destination folder\n",
        "    for image in images_to_move:\n",
        "        src_path = os.path.join(src_folder, image)  # Full path to the source image\n",
        "        dest_path = os.path.join(dest_folder, image)  # Full path to the destination\n",
        "        shutil.move(src_path, dest_path)  # Move the image\n",
        "\n",
        "    # Print a summary of the operation\n",
        "    print(f\"{num_images_to_move} images moved from {src_folder} to {dest_folder}\")\n",
        "\n",
        "# Move images for each grade\n",
        "# Iterate over the grade groups and their corresponding percentages\n",
        "for group, percentage in percentages.items():\n",
        "    group_train_dir = os.path.join(train_dir, group)  # Training folder for the current grade\n",
        "    group_test_dir = os.path.join(test_dir, group)  # Testing folder for the current grade\n",
        "\n",
        "    # Ensure the destination folder exists\n",
        "    if not os.path.exists(group_test_dir):\n",
        "        os.makedirs(group_test_dir)  # Create the folder if it doesn't exist\n",
        "\n",
        "    # Move the images for the current grade based on the specified percentage\n",
        "    move_images(group_train_dir, group_test_dir, percentage)\n"
      ],
      "metadata": {
        "id": "9U-lZ7WRWC34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESIZED IMAGES"
      ],
      "metadata": {
        "id": "ygvhCA9tkPyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RESIZE IMAGE PATCHES TO 512x512\n",
        "\n",
        "# Define the input and output folder paths for the patches\n",
        "# `patches_train_folder` contains the patches for the training set.\n",
        "# `patches_test_folder` contains the patches for the testing set.\n",
        "patches_train_folder = '/safo balanced final/trainpatches'\n",
        "patches_test_folder = '/safo balanced final/testpatches'\n",
        "\n",
        "# Define the output folder paths for the resized patches\n",
        "# `resized_train_folder` will store the resized patches for training.\n",
        "# `resized_test_folder` will store the resized patches for testing.\n",
        "resized_train_folder = '/safo balanced final/resizedtrainpatches'\n",
        "resized_test_folder = '/safo balanced final/resizedtestpatches'\n",
        "\n",
        "# Define the output size for the resized images (512x512)\n",
        "output_size = (512, 512)\n",
        "\n",
        "# Create the output folders if they don't already exist\n",
        "# Ensures that the required folders exist for resized training and testing patches.\n",
        "os.makedirs(resized_train_folder, exist_ok=True)\n",
        "os.makedirs(resized_test_folder, exist_ok=True)\n",
        "\n",
        "# Function to resize an image to the desired size\n",
        "def resize_image(image, size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Resizes the input image to the specified size.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array): The input image to be resized.\n",
        "        size (tuple): The target size (width, height) for the resized image.\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The resized image.\n",
        "    \"\"\"\n",
        "    # Use INTER_AREA interpolation method for resampling (better for shrinking images)\n",
        "    resized_image = cv2.resize(image, size, interpolation=cv2.INTER_AREA)\n",
        "    return resized_image\n",
        "\n",
        "# Function to resize all patches in a folder\n",
        "def resize_patches_in_folder(patch_folder, resized_folder):\n",
        "    \"\"\"\n",
        "    Iterates through all patches in the given folder, resizes them, and saves them to the resized folder.\n",
        "\n",
        "    Args:\n",
        "        patch_folder (str): Path to the folder containing image patches to be resized.\n",
        "        resized_folder (str): Path to the folder where the resized patches will be saved.\n",
        "    \"\"\"\n",
        "    # Iterate through each grade (0 to 3) and resize patches for each grade folder\n",
        "    for grade in range(4):\n",
        "        grade_folder = os.path.join(patch_folder, f'grade{grade}')\n",
        "        resized_grade_folder = os.path.join(resized_folder, f'grade{grade}')\n",
        "\n",
        "        # Create the subfolder for each grade if it does not exist\n",
        "        os.makedirs(resized_grade_folder, exist_ok=True)\n",
        "\n",
        "        # Iterate through all files in the current grade folder\n",
        "        for filename in os.listdir(grade_folder):\n",
        "            if filename.endswith(('.jpg', '.jpeg', '.tif', '.tiff', '.png')):  # Only process image files\n",
        "                patch_path = os.path.join(grade_folder, filename)\n",
        "\n",
        "                # Read the patch image\n",
        "                patch = cv2.imread(patch_path)\n",
        "\n",
        "                if patch is not None:\n",
        "                    # Resize the image using the resize_image function\n",
        "                    resized_patch = resize_image(patch, output_size)\n",
        "\n",
        "                    # Save the resized patch to the corresponding folder\n",
        "                    resized_patch_path = os.path.join(resized_grade_folder, f'resized_{filename}')\n",
        "                    cv2.imwrite(resized_patch_path, resized_patch)\n",
        "                    print(f'Resized and saved: {resized_patch_path}')\n",
        "                else:\n",
        "                    print(f'Error reading image: {patch_path}')\n",
        "\n",
        "# Resize the patches in both the training and testing datasets\n",
        "resize_patches_in_folder(patches_train_folder, resized_train_folder)\n",
        "resize_patches_in_folder(patches_test_folder, resized_test_folder)\n",
        "\n",
        "# Print a message indicating the process has completed\n",
        "print('All images have been resampled to 512x512 and saved to the resized folders.')\n"
      ],
      "metadata": {
        "id": "g6qW79zhWC6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFER LEARNING"
      ],
      "metadata": {
        "id": "fa9jE1tJotbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "AfACHdGPo5qD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE DATA FOLDERS AND CONFIGURE IMAGE DATA GENERATORS\n",
        "\n",
        "# Define the paths to the training and testing image directories\n",
        "# `train_dir` contains the resized training images.\n",
        "# `test_dir` contains the resized testing images.\n",
        "train_dir = '/dataset tfg final/resizedtrainpatches'\n",
        "test_dir = '/dataset tfg final/resizedtestpatches'\n",
        "\n",
        "# Configure the ImageDataGenerator for the training data\n",
        "# ImageDataGenerator is used to load and preprocess images from the directories, including data augmentation for training.\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,  # Randomly rotate images by up to 30 degrees\n",
        "    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20% of the image width\n",
        "    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of the image height\n",
        "    zoom_range=0.2,  # Randomly zoom in on images by up to 20%\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    vertical_flip=False,  # Do not flip images vertically\n",
        "    fill_mode='nearest',  # Fill in any missing pixels after transformation with the nearest pixel\n",
        "    preprocessing_function=preprocess_input  # Apply DenseNet's preprocessing to normalize the input data\n",
        ")\n",
        "\n",
        "# Configure the ImageDataGenerator for the testing data\n",
        "# For testing, we only rescale the images, and no augmentation is applied.\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input  # Apply DenseNet's preprocessing to normalize the input data\n",
        ")\n",
        "\n",
        "# Load the training images using the `train_datagen` generator\n",
        "# `train_generator` provides batches of augmented training images from the `train_dir`.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,  # Path to the training images directory\n",
        "    target_size=(512, 512),  # Resize all images to 512x512 pixels (required by DenseNet)\n",
        "    batch_size=32,  # Number of images to process in each batch\n",
        "    class_mode='categorical',  # Use categorical labels (one-hot encoding for multi-class classification)\n",
        "    shuffle=True  # Shuffle the images during training to ensure randomness\n",
        ")\n",
        "\n",
        "# Load the testing images using the `test_datagen` generator\n",
        "# `test_generator` provides batches of images from the `test_dir` for evaluation.\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,  # Path to the testing images directory\n",
        "    target_size=(512, 512),  # Resize all images to 512x512 pixels (required by DenseNet)\n",
        "    batch_size=32,  # Number of images to process in each batch\n",
        "    class_mode='categorical',  # Use categorical labels for evaluation (one-hot encoding)\n",
        "    shuffle=False  # Do not shuffle the images during testing (we want to evaluate the images in their original order)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdC7QJKiWDLS",
        "outputId": "8e039d42-b128-47da-d551-b1b401e8aad6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 114 images belonging to 4 classes.\n",
            "Found 147 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DENSENET121**"
      ],
      "metadata": {
        "id": "w28yhPhrozQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
        "from tensorflow.keras.applications import DenseNet121"
      ],
      "metadata": {
        "id": "MVCHQSFMtGoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILDING THE MODEL WITH DENSENET121 AS THE BASE\n",
        "\n",
        "# Load the pre-trained DenseNet121 model, excluding the top fully connected layer (include_top=False)\n",
        "# The 'weights' argument specifies that the model should be initialized with ImageNet weights.\n",
        "# The 'input_shape' specifies the shape of the input images (512x512 pixels with 3 color channels).\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(512, 512, 3))\n",
        "\n",
        "# The output of the base model will be passed to additional layers for custom classification\n",
        "x = base_model.output\n",
        "\n",
        "# Add a Global Average Pooling layer to reduce the spatial dimensions of the feature map\n",
        "# This helps reduce the number of parameters and the risk of overfitting.\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully connected (dense) layer with 1024 units and ReLU activation\n",
        "# This layer helps the model learn complex relationships and make predictions.\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Add the final output layer with the number of classes in the training set (num_classes),\n",
        "# using a softmax activation function to output probabilities for each class.\n",
        "# The output layer will have one unit per class in the classification task.\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the model by specifying the input and output layers\n",
        "# The model will use DenseNet121 as the feature extractor and the newly added layers for classification.\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n"
      ],
      "metadata": {
        "id": "oLXfT77XWDNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Without fine-tunning DenseNet121*"
      ],
      "metadata": {
        "id": "6j0mHYiHqgcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through all layers in the base model (DenseNet121) and set them as non-trainable\n",
        "# This is done to keep the pre-trained weights of the base model frozen and avoid modifying them during training.\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Set each layer in the base model to not be trainable\n"
      ],
      "metadata": {
        "id": "smLRARz3WDQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPILE THE MODEL WITH SPECIFIED OPTIMIZER, LOSS FUNCTION, AND METRICS\n",
        "\n",
        "# Compile the model by specifying the optimizer, loss function, and evaluation metrics\n",
        "# The optimizer is responsible for adjusting the model's weights based on the loss function during training.\n",
        "# The Adam optimizer is a popular adaptive learning rate method for deep learning models.\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),  # Adam optimizer with a learning rate of 0.001\n",
        "    loss='categorical_crossentropy',  # Loss function for multi-class classification (used when classes are mutually exclusive)\n",
        "    metrics=['accuracy']  # Track accuracy during training as the performance metric\n",
        ")\n"
      ],
      "metadata": {
        "id": "edN0n3MNWDTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING THE MODEL WITHOUT FINE-TUNING DENSENET121\n",
        "\n",
        "# Train the model using the training data generator and validate using the test data generator\n",
        "# The `fit()` function trains the model on the provided dataset for a specified number of epochs.\n",
        "# In this case, the model is trained without fine-tuning the DenseNet121 base model, meaning the pre-trained layers are frozen.\n",
        "history = model.fit(\n",
        "    train_generator,  # The generator that provides the training data (images and labels)\n",
        "    epochs=15,  # Number of epochs (iterations over the entire dataset)\n",
        "    validation_data=test_generator,  # The validation data to evaluate the model after each epoch\n",
        ")\n"
      ],
      "metadata": {
        "id": "NafsOCH9AwVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The file format used is HDF5 (.h5), which is commonly used for saving Keras models.\n",
        "model.save(f'E15model_sin_finetunningdensenet121.h5')  # Save the model\n",
        "with open('E15history_sin_finetunningdensenet121.json', 'w') as f:\n",
        "    json.dump(history.history, f)"
      ],
      "metadata": {
        "id": "wONA0Gz4WDYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*With fine-tunning DenseNet121*"
      ],
      "metadata": {
        "id": "X0Q89bdwqr8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING THE MODEL WITH FINE-TUNING DENSENET121\n",
        "\n",
        "# Freeze the first 108 layers of the base model (DenseNet121) to keep their pre-trained weights\n",
        "# By freezing the first layers, we allow the model to retain the learned features from ImageNet without updating these weights.\n",
        "for layer in base_model.layers[:108]:  # Freeze the first 108 layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Unfreeze the layers from 108 onwards, allowing them to be trained\n",
        "# These layers will be fine-tuned to adjust to the specific task (classification of osteoarthritis grades).\n",
        "for layer in base_model.layers[108:]:  # Unfreeze layers starting from layer 109\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model again after unfreezing layers\n",
        "# Adam optimizer with a lower learning rate to fine-tune the model without drastically changing the pre-trained features\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),  # Adam optimizer with a smaller learning rate for fine-tuning\n",
        "              loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
        "              metrics=['accuracy'])  # Track accuracy during training\n",
        "\n",
        "# Train the model with fine-tuning\n",
        "# `history_fine` stores the training history, including loss and accuracy metrics during training.\n",
        "history_fine = model.fit(\n",
        "    train_generator,  # The generator that provides the training data (images and labels)\n",
        "    epochs=15,  # Number of epochs (iterations over the entire dataset)\n",
        "    validation_data=test_generator,  # The validation data to evaluate the model after each epoch\n",
        ")\n"
      ],
      "metadata": {
        "id": "WioM1h6gWDa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'model.save()' function is used to save the entire model, including its architecture, weights, and training configuration, to a file.\n",
        "# The file format used is HDF5 (.h5), which is commonly used for saving Keras models.\n",
        "model.save(f'E15model_con_finetunningdensenet121.h5')  # Save the model with the name 'model1.h5'\n",
        "with open('E15history_con_finetunningdensenet121.json', 'w') as f:\n",
        "    json.dump(history_fine.history, f)"
      ],
      "metadata": {
        "id": "OxinlJsCWDdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESNET50**"
      ],
      "metadata": {
        "id": "oBvCRtK1syWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n"
      ],
      "metadata": {
        "id": "xH-crCE1q8DO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(512, 512, 3)) #The model was tried without transfer learning, but no results were obtained (weights=None)"
      ],
      "metadata": {
        "id": "4VIYIvFGq8GO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "NkFSpii0q8J3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "-qqT4fIUq8Mr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "SyGL2AAqq8Ps"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*WITHOUT FINE TUNNING RESNET50*"
      ],
      "metadata": {
        "id": "-_B7VyzftsyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without fine tunning ResNet50\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=test_generator,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrX3L82jq8TH",
        "outputId": "d863102a-95db-4225-ec0f-ff7d6ea5958f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 75s/step - accuracy: 0.1739 - loss: 4.0178 - val_accuracy: 0.5374 - val_loss: 2.8856\n",
            "Epoch 2/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 74s/step - accuracy: 0.2531 - loss: 3.5925 - val_accuracy: 0.3741 - val_loss: 2.9097\n",
            "Epoch 3/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 73s/step - accuracy: 0.5390 - loss: 1.9174 - val_accuracy: 0.4694 - val_loss: 1.2407\n",
            "Epoch 4/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 74s/step - accuracy: 0.7186 - loss: 0.6457 - val_accuracy: 0.5578 - val_loss: 1.0972\n",
            "Epoch 5/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 73s/step - accuracy: 0.6514 - loss: 0.9012 - val_accuracy: 0.5578 - val_loss: 1.4715\n",
            "Epoch 6/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 77s/step - accuracy: 0.8243 - loss: 0.4599 - val_accuracy: 0.4966 - val_loss: 1.4049\n",
            "Epoch 7/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 74s/step - accuracy: 0.8486 - loss: 0.4701 - val_accuracy: 0.6667 - val_loss: 1.0978\n",
            "Epoch 8/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 79s/step - accuracy: 0.8746 - loss: 0.3561 - val_accuracy: 0.6122 - val_loss: 1.3869\n",
            "Epoch 9/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 79s/step - accuracy: 0.9005 - loss: 0.2846 - val_accuracy: 0.5782 - val_loss: 1.4215\n",
            "Epoch 10/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 77s/step - accuracy: 0.9140 - loss: 0.2612 - val_accuracy: 0.6871 - val_loss: 1.0933\n",
            "Epoch 11/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 73s/step - accuracy: 0.8846 - loss: 0.2616 - val_accuracy: 0.6122 - val_loss: 1.6735\n",
            "Epoch 12/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 74s/step - accuracy: 0.9447 - loss: 0.1914 - val_accuracy: 0.6259 - val_loss: 1.5372\n",
            "Epoch 13/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 94s/step - accuracy: 0.9490 - loss: 0.1865 - val_accuracy: 0.6803 - val_loss: 1.1422\n",
            "Epoch 14/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 75s/step - accuracy: 0.9312 - loss: 0.2079 - val_accuracy: 0.6259 - val_loss: 1.6422\n",
            "Epoch 15/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 74s/step - accuracy: 0.9516 - loss: 0.1384 - val_accuracy: 0.6327 - val_loss: 1.4685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'model.save()' function is used to save the entire model, including its architecture, weights, and training configuration, to a file.\n",
        "# The file format used is HDF5 (.h5), which is commonly used for saving Keras models.\n",
        "model.save(f'A15model_sin_finetunningResNet50.h5')  # Save the model with the name 'model1.h5'\n",
        "with open('A15history_sin_finetunningResNet50.json', 'w') as f:\n",
        "    json.dump(history.history, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FTvvi5gt855",
        "outputId": "d7e852ed-9c82-499f-b358-500a5810b867"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*WITH FINE TUNNING RESNET50*"
      ],
      "metadata": {
        "id": "iDwD0Xjot3jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING THE MODEL WITH FINE-TUNING RESNET50\n",
        "\n",
        "# Freeze the first 45 layers of the base model (ResNet50) to keep their pre-trained weights\n",
        "# By freezing the first layers, we retain the learned features from ImageNet without modifying these weights.\n",
        "for layer in base_model.layers[:45]:  # Freeze the first 45 layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Unfreeze the layers from 45 onwards, allowing them to be trained\n",
        "# These layers will be fine-tuned to adapt to the specific classification task (osteoarthritis grading).\n",
        "for layer in base_model.layers[45:]:  # Unfreeze layers starting from layer 46\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model again after unfreezing layers\n",
        "# Adam optimizer with a lower learning rate to fine-tune the model without drastically changing the pre-trained features\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),  # Adam optimizer with a small learning rate for fine-tuning\n",
        "              loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
        "              metrics=['accuracy'])  # Track accuracy during training\n",
        "\n",
        "# Train the model with fine-tuning\n",
        "# `history_fine` stores the training history, including loss and accuracy metrics during training.\n",
        "history_fine = model.fit(\n",
        "    train_generator,  # The generator that provides the training data (images and labels)\n",
        "    epochs=15,  # Number of epochs (iterations over the entire dataset)\n",
        "    validation_data=test_generator,  # The validation data to evaluate the model after each epoch\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jlPr09DWDi9",
        "outputId": "d0fb6ab5-b4f3-4bb2-f53d-dc5f9783ad2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 131s/step - accuracy: 0.5504 - loss: 2.3481 - val_accuracy: 0.1633 - val_loss: 7.4621\n",
            "Epoch 2/15\n",
            "\u001b[1m2/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:12\u001b[0m 96s/step - accuracy: 0.8289 - loss: 0.3623"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'model.save()' function is used to save the entire model, including its architecture, weights, and training configuration, to a file.\n",
        "# The file format used is HDF5 (.h5), which is commonly used for saving Keras models.\n",
        "model.save(f'M15modelcon_finetunningResNet50.h5')  # Save the model with the name 'model1.h5'\n",
        "with open('M15history_con_finetunningResNet50.json', 'w') as f:\n",
        "    json.dump(history_fine.history, f)"
      ],
      "metadata": {
        "id": "CRl_35V9uM3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EFFICIENTNETV2B0**"
      ],
      "metadata": {
        "id": "dA-JmZZsub8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input"
      ],
      "metadata": {
        "id": "RLNCg9wauM6C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained EfficientNetV2B0 model, excluding the top fully connected layer (include_top=False)\n",
        "# The 'weights' argument specifies that the model should be initialized with ImageNet weights.\n",
        "# The 'input_shape' specifies the shape of the input images (512x512 pixels with 3 color channels).\n",
        "base_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(512, 512, 3))  #The model was tried without transfer learning, but no results were obtained (weights=None)\n",
        "\n",
        "# The output of the base model will be passed to additional layers for custom classification\n",
        "x = base_model.output\n",
        "\n",
        "# Add a Global Average Pooling layer to reduce the spatial dimensions of the feature map\n",
        "# This helps reduce the number of parameters and prevent overfitting.\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully connected (dense) layer with 1024 units and ReLU activation\n",
        "# This layer helps the model learn complex relationships from the features extracted by the base model.\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Add the final output layer with the number of classes in the training set (num_classes),\n",
        "# using a softmax activation function to output probabilities for each class.\n",
        "# This layer will predict the class of each image based on the features learned.\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)  # Number of classes taken from the train folder\n",
        "\n",
        "# Create the model by specifying the input and output layers\n",
        "# The model uses EfficientNetV2B0 as the feature extractor and the added layers for classification.\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "phVemELeuM8g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "ZMRjcO3GuM-4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "t4Vn8if5uNBB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*WITHOUT FINE TUNNING EFFICIENTNETV2B0*"
      ],
      "metadata": {
        "id": "ZQpGOEtzvBMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#without fine tunning\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=test_generator,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TFBk5kOuNDj",
        "outputId": "bf5d3c8c-8c58-4b5e-95c7-e14d01a3cb69"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 32s/step - accuracy: 0.4562 - loss: 1.2164 - val_accuracy: 0.6259 - val_loss: 0.7387\n",
            "Epoch 2/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 37s/step - accuracy: 0.7423 - loss: 0.6070 - val_accuracy: 0.4422 - val_loss: 1.5362\n",
            "Epoch 3/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 28s/step - accuracy: 0.7482 - loss: 0.6546 - val_accuracy: 0.6871 - val_loss: 0.7094\n",
            "Epoch 4/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 26s/step - accuracy: 0.7653 - loss: 0.4892 - val_accuracy: 0.6599 - val_loss: 0.7490\n",
            "Epoch 5/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 37s/step - accuracy: 0.9172 - loss: 0.2487 - val_accuracy: 0.6395 - val_loss: 1.0290\n",
            "Epoch 6/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 32s/step - accuracy: 0.9323 - loss: 0.2552 - val_accuracy: 0.7211 - val_loss: 0.7339\n",
            "Epoch 7/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 28s/step - accuracy: 0.9307 - loss: 0.1697 - val_accuracy: 0.7143 - val_loss: 0.7665\n",
            "Epoch 8/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 37s/step - accuracy: 0.9198 - loss: 0.2171 - val_accuracy: 0.5646 - val_loss: 1.1988\n",
            "Epoch 9/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 29s/step - accuracy: 0.9342 - loss: 0.1691 - val_accuracy: 0.7415 - val_loss: 0.8562\n",
            "Epoch 10/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 36s/step - accuracy: 0.9746 - loss: 0.1287 - val_accuracy: 0.7347 - val_loss: 0.9083\n",
            "Epoch 11/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 40s/step - accuracy: 0.9447 - loss: 0.1404 - val_accuracy: 0.6190 - val_loss: 1.3108\n",
            "Epoch 12/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 26s/step - accuracy: 0.9739 - loss: 0.1389 - val_accuracy: 0.7279 - val_loss: 0.9952\n",
            "Epoch 13/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 27s/step - accuracy: 0.9355 - loss: 0.1299 - val_accuracy: 0.7143 - val_loss: 1.0633\n",
            "Epoch 14/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 27s/step - accuracy: 0.9870 - loss: 0.0835 - val_accuracy: 0.7075 - val_loss: 1.1567\n",
            "Epoch 15/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 37s/step - accuracy: 0.9719 - loss: 0.0778 - val_accuracy: 0.6395 - val_loss: 1.4149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'model.save()' function is used to save the entire model, including its architecture, weights, and training configuration, to a file.\n",
        "# The file format used is HDF5 (.h5), which is commonly used for saving Keras models.\n",
        "model.save(f'M15model_sin_finetunningrEfficientnetv2b0.h5')  # Save the model with the name 'model1.h5'\n",
        "with open('M15history_sin_finetunningrEfficientnetv2b0.json', 'w') as f:\n",
        "    json.dump(history.history, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl2OBgMYvGti",
        "outputId": "a83c6ef5-705b-4266-bfa5-681fa9bd5bd2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*WITH FINE TUNNING EFFICIENTNETV2B0*"
      ],
      "metadata": {
        "id": "-GDxyjPkvKVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FINE-TUNING THE EFFICIENTNETV2B0 MODEL\n",
        "\n",
        "# Freeze the first 245 layers of the base model (EfficientNetV2B0) to retain their pre-trained weights\n",
        "# By freezing the first layers, we ensure that the low-level features (such as edges, textures, etc.)\n",
        "# learned from ImageNet are not modified during training.\n",
        "for layer in base_model.layers[:245]:  # Freeze the first 245 layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Unfreeze the layers from 245 onwards, allowing them to be trained\n",
        "# These layers will be fine-tuned to adjust to the specific classification task (such as the osteoarthritis grading).\n",
        "for layer in base_model.layers[245:]:  # Unfreeze layers from layer 246 onwards\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model again after unfreezing layers\n",
        "# Using the Adam optimizer with a smaller learning rate (0.0001) to fine-tune the model.\n",
        "# This allows the model to make small adjustments to the pre-trained features while learning new features for the task.\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),  # Adam optimizer with a small learning rate for fine-tuning\n",
        "              loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
        "              metrics=['accuracy'])  # Track accuracy during training\n",
        "\n",
        "# Train the model with fine-tuning\n",
        "# `history_fine` stores the training history, including the loss and accuracy metrics during training.\n",
        "history_fine = model.fit(\n",
        "    train_generator,  # The generator that provides the training data (images and labels)\n",
        "    epochs=15,  # Number of epochs (iterations over the entire dataset)\n",
        "    validation_data=test_generator,  # The validation data to evaluate the model after each epoch\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIhNYmkLvGwy",
        "outputId": "cfb13a0c-39d9-4be9-c8d6-a76f34352acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 30s/step - accuracy: 0.8341 - loss: 0.5044 - val_accuracy: 0.5986 - val_loss: 1.4626\n",
            "Epoch 2/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 38s/step - accuracy: 0.9351 - loss: 0.2310 - val_accuracy: 0.6122 - val_loss: 1.3821\n",
            "Epoch 3/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 28s/step - accuracy: 0.9482 - loss: 0.2212 - val_accuracy: 0.6122 - val_loss: 1.2626\n",
            "Epoch 4/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 28s/step - accuracy: 0.9585 - loss: 0.1353 - val_accuracy: 0.6327 - val_loss: 1.1664\n",
            "Epoch 5/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - accuracy: 0.9700 - loss: 0.1427 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'model.save()' function is used to save the entire model, including its architecture, weights, and training configuration, to a file.\n",
        "# The file format used is HDF5 (.h5), which is commonly used for saving Keras models.\n",
        "model.save(f'E15model_con_finetunningrEfficientnetv2b0.h5')  # Save the model with the name 'model1.h5'\n",
        "with open('E15history_con_finetunningrEfficientnetv2b0.json', 'w') as f:\n",
        "    json.dump(history.history, f)"
      ],
      "metadata": {
        "id": "4nY1uRC6vGzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}